{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Analysing Text Similarity**"
      ],
      "metadata": {
        "id": "FIE0liood-xq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN47hLFxdxSD",
        "outputId": "372ade9d-0a12-4ca5-a0be-1d225ca30cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import gensim\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample word tokenize\n",
        "data = \" Mars is approximately half the diameter of earth.\"\n",
        "print(\"word tokens\\n\",word_tokenize(data))\n",
        "data = \"Mars is a cold desert world.It is half the size of Earth\"\n",
        "print('sentenece tokens\\n',sent_tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLjuYHMWeM81",
        "outputId": "863ab672-f3c4-4c58-960e-38317c729bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word tokens\n",
            " ['Mars', 'is', 'approximately', 'half', 'the', 'diameter', 'of', 'earth', '.']\n",
            "sentenece tokens\n",
            " ['Mars is a cold desert world.It is half the size of Earth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#base file processing\n",
        "print(\"base file reading\")\n",
        "file_docs = []\n",
        "with open('demofile.txt') as f:\n",
        "  tokens = sent_tokenize(f.read())\n",
        "  for line in tokens:\n",
        "    file_docs.append(line)\n",
        "  print(\"no. of statements in base file : \",len(file_docs))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuhYrlSPePbr",
        "outputId": "f78e4f42-6265-4644-87aa-8dc286ebfa91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base file reading\n",
            "no. of statements in base file :  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization ofbase file\n",
        "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in file_docs]\n",
        "print(\"tokenized words \\n\",gen_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG8X5OyPePeR",
        "outputId": "a19d2ba5-e159-4864-f37b-70b383c00e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized words \n",
            " [['mars', 'is', 'the', 'fourth', 'planet', 'in', 'the', 'solar', 'system', '.'], ['it', 'is', 'the', 'second-smallest', 'planet', 'in', 'the', 'solar', 'system', 'after', 'mercury', '.'], ['saturn', 'is', 'yellow', 'planet', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating of dictionary\n",
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "print(\"tokens in dict \\n\",dictionary.token2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOiQi8QwePgl",
        "outputId": "7ac1941d-5f4d-49e6-d419-c104771d0cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens in dict \n",
            " {'.': 0, 'fourth': 1, 'in': 2, 'is': 3, 'mars': 4, 'planet': 5, 'solar': 6, 'system': 7, 'the': 8, 'after': 9, 'it': 10, 'mercury': 11, 'second-smallest': 12, 'saturn': 13, 'yellow': 14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary"
      ],
      "metadata": {
        "id": "Ba8kIPucePkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#corpus creation\n",
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "print('corpus\\n',corpus)\n",
        "#calculation of frequency\n",
        "\n",
        "print('TF-IDF')\n",
        "tf_idf = TfidfModel(corpus)  \n",
        "for doc in tf_idf[corpus]:\n",
        "  print([[dictionary[id],np.around(freq,decimals=2)] for id,freq in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5p2qOQXePnj",
        "outputId": "6e73d245-961f-45cd-fb32-9f56fb0193c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus\n",
            " [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2)], [(0, 1), (2, 1), (3, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1)], [(0, 1), (3, 1), (5, 1), (13, 1), (14, 1)]]\n",
            "TF-IDF\n",
            "[['fourth', 0.58], ['in', 0.21], ['mars', 0.58], ['solar', 0.21], ['system', 0.21], ['the', 0.43]]\n",
            "[['in', 0.17], ['solar', 0.17], ['system', 0.17], ['the', 0.33], ['after', 0.45], ['it', 0.45], ['mercury', 0.45], ['second-smallest', 0.45]]\n",
            "[['saturn', 0.71], ['yellow', 0.71]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#calculating similarity measure\n",
        "sims = gensim.similarities.Similarity('',tf_idf[corpus],num_features=len(dictionary))"
      ],
      "metadata": {
        "id": "xOXvPXm1e1bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#processing query file\n",
        "file2_docs = []\n",
        "with open('demofile2.txt') as f:\n",
        "  tokens = sent_tokenize(f.read())\n",
        "  for line in tokens:\n",
        "    file2_docs.append(line)\n",
        "print(\"no. of documents:\",len(file2_docs))\n",
        "for line in file2_docs:\n",
        "  query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "  query_doc_bow = dictionary.doc2bow(query_doc)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSbLTz3Le2oZ",
        "outputId": "bf2fc258-ac23-4d4c-88eb-80ca1e4dbf65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no. of documents: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#processing query file\n",
        "file2_docs = []\n",
        "with open('demofile2.txt') as f:\n",
        "  tokens = sent_tokenize(f.read())\n",
        "  for line in tokens:\n",
        "    file2_docs.append(line)\n",
        "print(\"no. of documents:\",len(file2_docs))\n",
        "for line in file2_docs:\n",
        "  query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "  query_doc_bow = dictionary.doc2bow(query_doc)\n",
        "  query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "  sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32)) \n",
        "  #Calculating Total Similarity\n",
        "  sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32)) \n",
        "  print('Sum of Similarity: ',sum_of_sims)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JNWts0re2Sk",
        "outputId": "685ce69d-ae5d-40b0-be85-9dd069aaea90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no. of documents: 1\n",
            "Sum of Similarity:  1.0209424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "A_McWmuge97x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform a similarity query afainds the corpus\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32)) \n",
        "#Calculating Total Similarity\n",
        "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32)) \n",
        "print('Sum of Similarity: ',sum_of_sims)\n",
        "\n",
        "print(\"sum of similarity: \",sum_of_sims)\n",
        "print('Sum of Similarity: ',sum_of_sims)\n",
        "#Calculating Similarity Values\n",
        "percentage_of_similarity= round(float( (sum_of_sims / len(file_docs)) * 100)) \n",
        "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
        " \n",
        "print(f'Average similarity percentage:  {float(sum_of_sims / len(file_docs)) * 100}')\n",
        " \n",
        "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmIyeYbifArn",
        "outputId": "0670b6ea-d218-45c1-c571-70b9169020d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of Similarity:  1.0209424\n",
            "sum of similarity:  1.0209424\n",
            "Sum of Similarity:  1.0209424\n",
            "Average similarity float: 0.3403141498565674\n",
            "Average similarity percentage:  34.03141498565674\n",
            "Average similarity rounded percentage: 34\n"
          ]
        }
      ]
    }
  ]
}